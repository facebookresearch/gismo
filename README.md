# InverseCooking 2.0

Code for inversecooking2.0: merges image-to-set prediction with previous inversecooking and adds functionalities to move towards multi-modal generation.

<br>

## Installation

This code uses Python 3.8.5 (Anaconda), PyTorch 1.7, Torchvision 0.8.1 and CUDA version 10.1.

Install PyTorch:

    conda install -c pytorch pytorch=1.7.1 torchvision cudatoolkit=10.2
    conda install -c conda-forge -c fvcore -c iopath fvcore iopath
    conda install pytorch3d -c pytorch3d

Install dependencies:

    pip install -r requirements.txt --upgrade

Additional dependencies for developers (optional):

    pip install -r requirements_dev.txt --upgrade
    pip install -e ".[dev]"

Seting up NLTK punkt and Spacy (for pre-processing of datasets):

    python -c "import nltk; nltk.download('punkt')"
    python3 -m spacy download en_core_web_lg

Verify that the install worked:

    python -c "import spacy; spacy.load('en_core_web_lg')"

<br>

## Datasets

#### Recipe1M

Download [Recipe1M](http://im2recipe.csail.mit.edu/dataset/download) (registration required) and extract under ```/path/to/recipe1m/```.

The contents of ```/path/to/recipe1m/``` should be the following:

```
det_ingrs.json
layer1.json
layer2.json
images/
images/train
images/val
images/test
```

Link the dataset to your current folder (the other option is to modify "path" in the configuration of the dataset)

    ln -s /path/to/recipe1m/ data/recipe1m

Pre-process the dataset with:

    python preprocess.py dataset=recipe1m

<br>

## Training

### Running experiments

Training can be run as in the following example command:

    python train.py task=im2recipe name=im2recipe

This command will look for the definition of the experiment "im2recipe" in the configuration
file "conf/experiments/im2recipe.yaml" and run this experiment.

Evaluation can be run as in the following example command:

    python eval.py task=im2recipe name=im2recipe

### Running on SLURM

Running on SLURM requires only to add the SLURM configuration to the command line:

    python train.py task=im2recipe name=im2recipe slurm=<SLURM_CONF>

Existing SLURM configurations can be found in the folder `conf/slurm/<SLURM_CONF>.yaml`.
Feel free to add configurations for your specific cluster.
You can find an example with `conf/slurm/devlab.yaml`.

### Monitoring progress

Check training progress with Tensorboard from the folder in which the checkpoint are saved:

    tensorboard --logdir='.' --port=6006

<br>

## Reproducing experiments

To train our best ViT-based model, follow the steps below:

Train the image-to-ingredient model:

    python train.py task=im2ingr name=im2ingr_vit_16_ff_bce_cat_multi_level

Once the job is finished, note the name of the folder in which it has been trained, for instance:

    PATH=/path/to/im2ingr-im2ingr_vit_16_ff_bce_cat_multi_level

Then train the image-to-recipe model using the checkpoint of the image-to-ingredient model to initialize the image encoder:

    python train.py task=im2recipe \
        name=im2recipe_vit16_multi_level \
        experiments.im2recipe.im2recipe_vit16_multi_level.pretrained_im2ingr.load_pretrained_from=PATH

Now, you can evaluate the end-to-end pipeline from image to predicted ingredients and generated recipe like so:

    python eval.py task=im2recipe \
        dataset.eval_split=val_all \
        name=im2recipe_vit16_multi_level_4 \
        eval_checkpoint_dir=/path/to/im2recipe_vit16_multi_level_4/best.ckpt

* Evaluate on `val_all` (full validation set)
* Using the checkpoint provided as parameter to initialize the model

You can customize the evaluation like so:

* Change evaluation set by changing the flag `eval_split` to one of: `train`, `val` (subset of 5k samples from val_all), `val_all`, and `test`.
* Use teacher forcing in the evaluation (for the ingredients) by setting `ingr_teachforce.test` to `True`

<br>

## Interacting with GISMo

[GISMo](./gismo/README.md) model allows to provide substitution suggestion. For each of the validation inputs, this model will write potential substitutions.

To evaluate the impact of substitutions generated by [GISMo](./gismo/README.md) on the image-to-recipe pipeline, you can run this command:

    python eval.py task=im2recipe \
        dataset.eval_split=val_all \
        name=im2recipe_vit16_multi_level_4 \
        eval_checkpoint_dir=/path/to/im2recipe_vit16_multi_level_4/best.ckpt \
        dataset.ablation.with_substitutions=True \
        dataset.ablation.alternate_substitution_set=/path/to/gismo_output/val_output.pkl

The last line will load the substitution of [GISMo](./gismo/README.md) and evaluate the perplexity and other metrics on the subset of the validation set that has substitutions.

<br>

## Pre-trained models

TBD

<br>

## Contributing

### Developer tools

The following commands should be used prior to submitting a fix:

- `make format` to format all the files with and remove useless imports
- `make test` to ensure that all unit tests are green
- `make check` to run basic static checks on the codebase

<br>

## License

TBD
