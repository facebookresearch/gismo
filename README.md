# InverseCooking 2.0

Code for inversecooking2.0: merges image-to-set prediction with previous inversecooking and adds functionalities to move towards multi-modal generation.

<br>

## Installation

This code uses Python 3.8.5 (Anaconda), PyTorch 1.7, Torchvision 0.8.1 and CUDA version 10.1.

Install PyTorch:

    conda install -c pytorch pytorch=1.7.1 torchvision cudatoolkit=10.2
    conda install -c conda-forge -c fvcore -c iopath fvcore iopath
    conda install pytorch3d -c pytorch3d

Install dependencies:

    pip install -r requirements.txt --upgrade

Additional dependencies for developers (optional):

    pip install -r requirements_dev.txt --upgrade
    pip install -e ".[dev]"

Seting up NLTK punkt and Spacy (for pre-processing of datasets):

    python -c "import nltk; nltk.download('punkt')"
    python3 -m spacy download en_core_web_lg

Verify that the install worked:

    python -c "import spacy; spacy.load('en_core_web_lg')"

<br>

## Datasets

#### Recipe1M

Download [Recipe1M](http://im2recipe.csail.mit.edu/dataset/download) (registration required) and extract under ```/path/to/recipe1m/```.

The contents of ```/path/to/recipe1m/``` should be the following:

```
det_ingrs.json
layer1.json
layer2.json
images/
images/train
images/val
images/test
```

Link the dataset to your current folder (the other option is to modify "path" in the configuration of the dataset)

    ln -s /path/to/recipe1m/ data/recipe1m

Pre-process the dataset with:

    python preprocess.py dataset=recipe1m

<br>

## Training

### Running experiments

Training can be run as in the following example command:

    python train.py task=im2recipe name=im2recipe

This command will look for the definition of the experiment "im2recipe" in the configuration
file "conf/experiments/im2recipe.yaml" and run this experiment.

Evaluation can be run as in the following example command:

    python eval.py task=im2recipe name=im2recipe

### Running on SLURM

Running on SLURM requires only to add the SLURM configuration to the command line:

    python train.py task=im2recipe name=im2recipe slurm=<SLURM_CONF>

Existing SLURM configurations can be found in the folder `conf/slurm/<SLURM_CONF>.yaml`.
Feel free to add configurations for your specific cluster.
You can find an example with `conf/slurm/devlab.yaml`.

### Monitoring progress

Check training progress with Tensorboard from the folder in which the checkpoint are saved:

    tensorboard --logdir='.' --port=6006

<br>

## Reproducing experiments

To train our best ViT-based model, and reproduce the numbers reported in the paper, follow the steps below.

### Training

Train the image-to-ingredient model:

    python train.py task=im2ingr name=im2ingr_vit_16_ff_bce_cat_multi_level

Once the job is finished, note the name of the folder in which it has been trained, for instance:

    PATH_1=/path/to/im2ingr-im2ingr_vit_16_ff_bce_cat_multi_level

Then train the image-to-recipe model using the checkpoint of the image-to-ingredient model to initialize the image encoder:

    python train.py task=im2recipe \
        name=im2recipe_vit16_multi_level \
        experiments.im2recipe.im2recipe_vit16_multi_level.pretrained_im2ingr.load_pretrained_from=<<PATH_1>>

Once the job is finished, note the name of the folder in which it has been trained, for instance:

    PATH_2=/path/to/im2recipe-im2recipe_vit16_multi_level_4

### Evaluation

Now, we can evaluate the model, first on the recipe generation alone (using ground truth ingredients):

    python eval.py task=im2recipe \
        dataset.eval_split=val_all \
        name=eval_im2recipe_vit16_multi_level_recipe_inference \
        eval_checkpoint_dir=<<PATH_2>>

The the end-to-end pipeline from image to predicted ingredients and generated recipe:

    python eval.py task=im2recipe \
        dataset.eval_split=val_all \
        name=eval_im2recipe_vit16_multi_level_full_inference \
        eval_checkpoint_dir=<<PATH_2>>

You can also customize the evaluation to use a different evaluation split.
To do so, change the flag `eval_split` in the command line to one of:
`train`, `val` (subset of 5k samples from val_all), `val_all` or `test`.

### Evaluation with GISMo

[GISMo](./gismo/README.md) model allows to provide substitution suggestion. For each of the validation inputs, this model will write potential substitutions.
To evaluate the impact of substitutions generated by [GISMo](./gismo/README.md) on the image-to-recipe pipeline, you can perform the following test.

First, measure the perplexity of recipes with no substitutions (this command sub-select the samples of the validate set that do have substitutions):

    python eval.py task=im2recipe \
        dataset.eval_split=val_all \
        name=eval_im2recipe_vit16_multi_level_recipe_inference \
        dataset.ablation.with_substitutions=True \
        eval_checkpoint_dir=<<PATH_2>>

Then, measure the perplexity of recipes with ground truth substitutions (on the same sub-set of the validation set):

    python eval.py task=im2recipe \
        dataset.eval_split=val_all \
        name=eval_im2recipe_vit16_multi_level_recipe_inference \
        dataset.ablation.with_substitutions=True \
        eval_checkpoint_dir=<<PATH_2>> \
        experiments.im2recipe.eval_im2recipe_vit16_multi_level_recipe_inference.ingr_teachforce.test=use_substitutions

Finally, measure the perplexity of recipes with GISMO substitutions (on the same sub-set of the validation set):

    python eval.py task=im2recipe \
        dataset.eval_split=val_all \
        name=eval_im2recipe_vit16_multi_level_recipe_inference \
        dataset.ablation.with_substitutions=True \
        eval_checkpoint_dir=<<PATH_2>> \
        experiments.im2recipe.eval_im2recipe_vit16_multi_level_recipe_inference.ingr_teachforce.test=use_substitutions \
        dataset.ablation.alternate_substitution_set=/path/to/gismo_output/val_output.pkl

<br>

### Visualisation with GISMo

The following command line with run some basic visualisation and export the model to a format with which interactive interaction with GISMo will be possible:

    python visualize.py task=im2recipe \
        dataset.ablation.with_substitutions=True \
        name=eval_im2recipe_vit16_multi_level_recipe_inference \
        eval_checkpoint_dir=<<PATH_2>>

After this command has completely, you should have a new `model_and_module.torch` file under the `PATH_2` folder.

Now, run `jupyter lab` and go to `notebooks/InteractiveGismo.ipynb` and use this path to interact with GISMo!


## Pre-trained models

TBD

<br>

## Contributing

### Developer tools

The following commands should be used prior to submitting a fix:

- `make format` to format all the files with and remove useless imports
- `make test` to ensure that all unit tests are green
- `make check` to run basic static checks on the codebase

<br>

## License

TBD
